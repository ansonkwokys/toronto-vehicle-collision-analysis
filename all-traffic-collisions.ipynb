{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16702120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 01:31:21 WARN Utils: Your hostname, KWOKs-MacBook-Air-2.local resolves to a loopback address: 127.0.0.1; using 10.0.0.179 instead (on interface en0)\n",
      "25/04/30 01:31:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/30 01:31:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae9368e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- crs: struct (nullable = true)\n",
      " |    |-- properties: struct (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- geometry: struct (nullable = true)\n",
      " |    |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- properties: struct (nullable = true)\n",
      " |    |    |    |-- AUTOMOBILE: string (nullable = true)\n",
      " |    |    |    |-- BICYCLE: string (nullable = true)\n",
      " |    |    |    |-- DIVISION: string (nullable = true)\n",
      " |    |    |    |-- FATALITIES: long (nullable = true)\n",
      " |    |    |    |-- FTR_COLLISIONS: string (nullable = true)\n",
      " |    |    |    |-- HOOD_158: string (nullable = true)\n",
      " |    |    |    |-- INJURY_COLLISIONS: string (nullable = true)\n",
      " |    |    |    |-- LAT_WGS84: string (nullable = true)\n",
      " |    |    |    |-- LONG_WGS84: string (nullable = true)\n",
      " |    |    |    |-- MOTORCYCLE: string (nullable = true)\n",
      " |    |    |    |-- NEIGHBOURHOOD_158: string (nullable = true)\n",
      " |    |    |    |-- OCC_DATE: string (nullable = true)\n",
      " |    |    |    |-- OCC_DOW: string (nullable = true)\n",
      " |    |    |    |-- OCC_HOUR: string (nullable = true)\n",
      " |    |    |    |-- OCC_MONTH: string (nullable = true)\n",
      " |    |    |    |-- OCC_YEAR: string (nullable = true)\n",
      " |    |    |    |-- PASSENGER: string (nullable = true)\n",
      " |    |    |    |-- PD_COLLISIONS: string (nullable = true)\n",
      " |    |    |    |-- PEDESTRIAN: string (nullable = true)\n",
      " |    |    |    |-- _id: long (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 01:35:47 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)/ 1]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.reflect.ManifestFactory$ObjectManifest.newArray(Manifest.scala:258)\n",
      "\tat scala.reflect.ManifestFactory$ObjectManifest.newArray(Manifest.scala:257)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofRef.mkArray(ArrayBuilder.scala:74)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofRef.resize(ArrayBuilder.scala:79)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofRef.sizeHint(ArrayBuilder.scala:84)\n",
      "\tat scala.collection.mutable.Builder.sizeHint(Builder.scala:82)\n",
      "\tat scala.collection.mutable.Builder.sizeHint$(Builder.scala:79)\n",
      "\tat scala.collection.mutable.ArrayBuilder.sizeHint(ArrayBuilder.scala:26)\n",
      "\tat scala.collection.TraversableLike.builder$1(TraversableLike.scala:282)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:285)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.catalyst.util.ResolveDefaultColumns$.getExistenceDefaultValues(ResolveDefaultColumnsUtil.scala:371)\n",
      "\tat org.apache.spark.sql.catalyst.util.ResolveDefaultColumns$.existenceDefaultValues(ResolveDefaultColumnsUtil.scala:444)\n",
      "\tat org.apache.spark.sql.catalyst.util.ResolveDefaultColumns$.applyExistenceDefaultValuesToRow(ResolveDefaultColumnsUtil.scala:414)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:489)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$19$1.applyOrElse(JacksonParser.scala:371)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$19$1.applyOrElse(JacksonParser.scala:370)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:415)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$19(JacksonParser.scala:370)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$2798/0x000000012ace6000.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:468)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$19$1.applyOrElse(JacksonParser.scala:371)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$19$1.applyOrElse(JacksonParser.scala:370)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:415)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$19(JacksonParser.scala:370)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$2798/0x000000012ace6000.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertArray(JacksonParser.scala:544)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$20$1.applyOrElse(JacksonParser.scala:377)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$20$1.applyOrElse(JacksonParser.scala:376)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:415)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$20(JacksonParser.scala:376)\n",
      "25/04/30 01:35:47 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 1.0 (TID 1),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.reflect.ManifestFactory$ObjectManifest.newArray(Manifest.scala:258)\n",
      "\tat scala.reflect.ManifestFactory$ObjectManifest.newArray(Manifest.scala:257)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofRef.mkArray(ArrayBuilder.scala:74)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofRef.resize(ArrayBuilder.scala:79)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofRef.sizeHint(ArrayBuilder.scala:84)\n",
      "\tat scala.collection.mutable.Builder.sizeHint(Builder.scala:82)\n",
      "\tat scala.collection.mutable.Builder.sizeHint$(Builder.scala:79)\n",
      "\tat scala.collection.mutable.ArrayBuilder.sizeHint(ArrayBuilder.scala:26)\n",
      "\tat scala.collection.TraversableLike.builder$1(TraversableLike.scala:282)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:285)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.catalyst.util.ResolveDefaultColumns$.getExistenceDefaultValues(ResolveDefaultColumnsUtil.scala:371)\n",
      "\tat org.apache.spark.sql.catalyst.util.ResolveDefaultColumns$.existenceDefaultValues(ResolveDefaultColumnsUtil.scala:444)\n",
      "\tat org.apache.spark.sql.catalyst.util.ResolveDefaultColumns$.applyExistenceDefaultValuesToRow(ResolveDefaultColumnsUtil.scala:414)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:489)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$19$1.applyOrElse(JacksonParser.scala:371)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$19$1.applyOrElse(JacksonParser.scala:370)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:415)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$19(JacksonParser.scala:370)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$2798/0x000000012ace6000.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:468)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$19$1.applyOrElse(JacksonParser.scala:371)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$19$1.applyOrElse(JacksonParser.scala:370)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:415)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$19(JacksonParser.scala:370)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$2798/0x000000012ace6000.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertArray(JacksonParser.scala:544)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$20$1.applyOrElse(JacksonParser.scala:377)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$20$1.applyOrElse(JacksonParser.scala:376)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:415)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$20(JacksonParser.scala:376)\n",
      "25/04/30 01:35:47 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (10.0.0.179 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.reflect.ManifestFactory$ObjectManifest.newArray(Manifest.scala:258)\n",
      "\tat scala.reflect.ManifestFactory$ObjectManifest.newArray(Manifest.scala:257)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofRef.mkArray(ArrayBuilder.scala:74)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofRef.resize(ArrayBuilder.scala:79)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofRef.sizeHint(ArrayBuilder.scala:84)\n",
      "\tat scala.collection.mutable.Builder.sizeHint(Builder.scala:82)\n",
      "\tat scala.collection.mutable.Builder.sizeHint$(Builder.scala:79)\n",
      "\tat scala.collection.mutable.ArrayBuilder.sizeHint(ArrayBuilder.scala:26)\n",
      "\tat scala.collection.TraversableLike.builder$1(TraversableLike.scala:282)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:285)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.catalyst.util.ResolveDefaultColumns$.getExistenceDefaultValues(ResolveDefaultColumnsUtil.scala:371)\n",
      "\tat org.apache.spark.sql.catalyst.util.ResolveDefaultColumns$.existenceDefaultValues(ResolveDefaultColumnsUtil.scala:444)\n",
      "\tat org.apache.spark.sql.catalyst.util.ResolveDefaultColumns$.applyExistenceDefaultValuesToRow(ResolveDefaultColumnsUtil.scala:414)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:489)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$19$1.applyOrElse(JacksonParser.scala:371)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$19$1.applyOrElse(JacksonParser.scala:370)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:415)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$19(JacksonParser.scala:370)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$2798/0x000000012ace6000.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:468)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$19$1.applyOrElse(JacksonParser.scala:371)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$19$1.applyOrElse(JacksonParser.scala:370)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:415)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$19(JacksonParser.scala:370)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$2798/0x000000012ace6000.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertArray(JacksonParser.scala:544)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$20$1.applyOrElse(JacksonParser.scala:377)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$20$1.applyOrElse(JacksonParser.scala:376)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:415)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$20(JacksonParser.scala:376)\n",
      "\n",
      "25/04/30 01:35:47 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ansonkwok/Projects/toronto-vehicle-collision-analysis/.venv/lib/python3.13/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ansonkwok/Projects/toronto-vehicle-collision-analysis/.venv/lib/python3.13/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/ansonkwok/Projects/toronto-vehicle-collision-analysis/.venv/lib/python3.13/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "        \"Error while sending or receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ansonkwok/Projects/toronto-vehicle-collision-analysis/.venv/lib/python3.13/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py\", line 719, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ~~~~~~~~~~~~~~~~~~~~^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ansonkwok/Projects/toronto-vehicle-collision-analysis/.venv/lib/python3.13/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/ansonkwok/Projects/toronto-vehicle-collision-analysis/.venv/lib/python3.13/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "        \"Error while sending or receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o28.showString",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m df = spark.read.option(\u001b[33m\"\u001b[39m\u001b[33mmultiline\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m).json(\u001b[33m\"\u001b[39m\u001b[33m./toronto-traffic-collisions.geojson\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m df.printSchema()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/toronto-vehicle-collision-analysis/.venv/lib/python3.13/site-packages/pyspark/sql/dataframe.py:947\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    888\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[32m    889\u001b[39m \n\u001b[32m    890\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    945\u001b[39m \u001b[33;03m    name | Bob\u001b[39;00m\n\u001b[32m    946\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/toronto-vehicle-collision-analysis/.venv/lib/python3.13/site-packages/pyspark/sql/dataframe.py:965\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    959\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    960\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    961\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    962\u001b[39m     )\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/toronto-vehicle-collision-analysis/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/toronto-vehicle-collision-analysis/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/toronto-vehicle-collision-analysis/.venv/lib/python3.13/site-packages/py4j/protocol.py:334\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    330\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    335\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    336\u001b[39m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name))\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28mtype\u001b[39m = answer[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mPy4JError\u001b[39m: An error occurred while calling o28.showString"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = spark.read.option(\"multiline\",\"true\").json(\"./toronto-traffic-collisions.geojson\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1197ce9f",
   "metadata": {},
   "source": [
    "We extracted the traffic collision json dataset to a dataframe on spark.\n",
    "We are going to conduct data cleaning on the dataframe.\n",
    "- split geometry and properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c75b569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------+-------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n",
      "|_corrupt_record                                                                      |geometry                                         |properties                                                                                                                                                                    |type   |\n",
      "+-------------------------------------------------------------------------------------+-------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n",
      "|{                                                                                    |NULL                                             |NULL                                                                                                                                                                          |NULL   |\n",
      "|\"type\": \"FeatureCollection\",                                                         |NULL                                             |NULL                                                                                                                                                                          |NULL   |\n",
      "|\"name\": \"Traffic Collisions - 4326\",                                                 |NULL                                             |NULL                                                                                                                                                                          |NULL   |\n",
      "|\"crs\": { \"type\": \"name\", \"properties\": { \"name\": \"urn:ogc:def:crs:OGC:1.3:CRS84\" } },|NULL                                             |NULL                                                                                                                                                                          |NULL   |\n",
      "|\"features\": [                                                                        |NULL                                             |NULL                                                                                                                                                                          |NULL   |\n",
      "|NULL                                                                                 |{[[-79.5631385029999, 43.674410633]], MultiPoint}|{YES, NO, D23, NULL, NO, 007, NO, 43.67441063320029, -79.56313850270357, NO, Willowridge-Martingrove-Richview (7), 1388552400000, Wednesday, 2, January, 2014, NO, YES, NO, 1}|Feature|\n",
      "|NULL                                                                                 |{[[-79.397589142, 43.726091178]], MultiPoint}    |{YES, NO, D32, NULL, NO, 105, NO, 43.72609117778053, -79.39758914167055, NO, Lawrence Park North (105), 1388552400000, Wednesday, 14, January, 2014, NO, YES, NO, 2}          |Feature|\n",
      "|NULL                                                                                 |{[[0.0, 0.0]], MultiPoint}                       |{YES, NO, NSA, NULL, NO, NSA, YES, 0, 0, NO, NSA, 1388552400000, Wednesday, 2, January, 2014, NO, NO, NO, 3}                                                                  |Feature|\n",
      "|NULL                                                                                 |{[[0.0, 0.0]], MultiPoint}                       |{YES, NO, NSA, NULL, NO, NSA, NO, 0, 0, NO, NSA, 1388552400000, Wednesday, 3, January, 2014, NO, YES, NO, 4}                                                                  |Feature|\n",
      "|NULL                                                                                 |{[[0.0, 0.0]], MultiPoint}                       |{YES, NO, NSA, NULL, NO, NSA, YES, 0, 0, NO, NSA, 1388552400000, Wednesday, 5, January, 2014, NO, NO, NO, 5}                                                                  |Feature|\n",
      "|NULL                                                                                 |{[[0.0, 0.0]], MultiPoint}                       |{YES, NO, NSA, NULL, NO, NSA, NO, 0, 0, NO, NSA, 1388552400000, Wednesday, 5, January, 2014, NO, YES, NO, 6}                                                                  |Feature|\n",
      "|NULL                                                                                 |{[[0.0, 0.0]], MultiPoint}                       |{YES, NO, NSA, NULL, NO, NSA, NO, 0, 0, NO, NSA, 1388552400000, Wednesday, 8, January, 2014, NO, YES, NO, 7}                                                                  |Feature|\n",
      "|NULL                                                                                 |{[[0.0, 0.0]], MultiPoint}                       |{YES, NO, NSA, NULL, NO, NSA, NO, 0, 0, NO, NSA, 1388552400000, Wednesday, 8, January, 2014, NO, YES, NO, 8}                                                                  |Feature|\n",
      "|NULL                                                                                 |{[[0.0, 0.0]], MultiPoint}                       |{YES, NO, NSA, NULL, NO, NSA, NO, 0, 0, NO, NSA, 1388552400000, Wednesday, 9, January, 2014, NO, YES, NO, 9}                                                                  |Feature|\n",
      "|NULL                                                                                 |{[[0.0, 0.0]], MultiPoint}                       |{YES, NO, NSA, NULL, NO, NSA, YES, 0, 0, NO, NSA, 1388552400000, Wednesday, 9, January, 2014, YES, NO, NO, 10}                                                                |Feature|\n",
      "|NULL                                                                                 |{[[0.0, 0.0]], MultiPoint}                       |{YES, NO, D55, NULL, NO, NSA, YES, 0, 0, NO, NSA, 1388552400000, Wednesday, 17, January, 2014, NO, NO, NO, 11}                                                                |Feature|\n",
      "|NULL                                                                                 |{[[0.0, 0.0]], MultiPoint}                       |{YES, NO, NSA, NULL, NO, NSA, YES, 0, 0, NO, NSA, 1388552400000, Wednesday, 17, January, 2014, NO, NO, NO, 12}                                                                |Feature|\n",
      "|NULL                                                                                 |{[[0.0, 0.0]], MultiPoint}                       |{YES, NO, D43, NULL, NO, NSA, NO, 0, 0, NO, NSA, 1388552400000, Wednesday, 19, January, 2014, NO, YES, NO, 13}                                                                |Feature|\n",
      "|NULL                                                                                 |{[[-79.434712522, 43.637455984]], MultiPoint}    |{YES, NO, D14, NULL, NO, 085, NO, 43.6374559839486, -79.4347125217612, NO, South Parkdale (85), 1388552400000, Wednesday, 19, January, 2014, NO, YES, NO, 14}                 |Feature|\n",
      "|NULL                                                                                 |{[[-79.3148386189999, 43.66232878]], MultiPoint} |{YES, NO, D55, NULL, NO, 070, NO, 43.66232877988148, -79.31483861853935, NO, South Riverdale (70), 1388552400000, Wednesday, 17, January, 2014, NO, YES, NO, 15}              |Feature|\n",
      "+-------------------------------------------------------------------------------------+-------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d7ca7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# check if only the first few rows of the first column contain the corrupted data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_corrupt_record\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# df2 = spark.read.schema(schema).json(\"./toronto-traffic-collisions.geojson\").cache()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/toronto-vehicle-collision-analysis/.venv/lib/python3.13/site-packages/pyspark/sql/dataframe.py:947\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    888\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[32m    889\u001b[39m \n\u001b[32m    890\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    945\u001b[39m \u001b[33;03m    name | Bob\u001b[39;00m\n\u001b[32m    946\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/toronto-vehicle-collision-analysis/.venv/lib/python3.13/site-packages/pyspark/sql/dataframe.py:965\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    959\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    960\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    961\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    962\u001b[39m     )\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/toronto-vehicle-collision-analysis/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/toronto-vehicle-collision-analysis/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count()."
     ]
    }
   ],
   "source": [
    "# check if only the first few rows of the first column contain the corrupted data\n",
    "df.select(\"_corrupt_record\").show()\n",
    "# df2 = spark.read.schema(schema).json(\"./toronto-traffic-collisions.geojson\").cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
